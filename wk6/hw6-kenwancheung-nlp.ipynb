{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6 - NLP - Kenwan Cheung\n",
    "\n",
    "You have been provided with a pickle file, containing 100 news articles about some company.  Use appropriate topic modeling technique to identify top N most important topics.\n",
    "\n",
    "read_pickle(directory+news_03.pkl')\n",
    "Present top N most important topics in these news articles\n",
    "Select N to identify relevant topics, but minimize duplication\n",
    "Explain how you selected N\n",
    "Rules and requirements:\n",
    "\n",
    "Your final output and the code should be contained within Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Boog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "news = pd.read_pickle('../wk6/news_toyota.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawled</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-02T04:24:51.072+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>QR Code Link to This Post All maintenance rece...</td>\n",
       "      <td>Dependable truck 03 Toyota Tacoma Double Cab $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-02T04:27:15.000+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>0 \\nNEW YORK: Automakers reported mixed US car...</td>\n",
       "      <td>US car sales mixed in January; trucks stay strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-02T04:34:00.008+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>transmission: automatic   2005 Toyota Camry LE...</td>\n",
       "      <td>2005 TOYOTA CAMRY LE 167300 MILEAGE $2450 (TAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-02T04:36:42.006+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>favorite this post Brand New Toyota Avalon Flo...</td>\n",
       "      <td>Brand New Toyota Avalon Floor Mats (New Britai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-02T04:38:24.018+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>more ads by this user QR Code Link to This Pos...</td>\n",
       "      <td>2016 Lexus ES 350 (Coliseum Lexus of Oakland) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         crawled language  \\\n",
       "0  2018-02-02T04:24:51.072+02:00  english   \n",
       "1  2018-02-02T04:27:15.000+02:00  english   \n",
       "2  2018-02-02T04:34:00.008+02:00  english   \n",
       "3  2018-02-02T04:36:42.006+02:00  english   \n",
       "4  2018-02-02T04:38:24.018+02:00  english   \n",
       "\n",
       "                                                text  \\\n",
       "0  QR Code Link to This Post All maintenance rece...   \n",
       "1  0 \\nNEW YORK: Automakers reported mixed US car...   \n",
       "2  transmission: automatic   2005 Toyota Camry LE...   \n",
       "3  favorite this post Brand New Toyota Avalon Flo...   \n",
       "4  more ads by this user QR Code Link to This Pos...   \n",
       "\n",
       "                                               title  \n",
       "0  Dependable truck 03 Toyota Tacoma Double Cab $...  \n",
       "1  US car sales mixed in January; trucks stay strong  \n",
       "2  2005 TOYOTA CAMRY LE 167300 MILEAGE $2450 (TAL...  \n",
       "3  Brand New Toyota Avalon Floor Mats (New Britai...  \n",
       "4  2016 Lexus ES 350 (Coliseum Lexus of Oakland) ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['english'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(news.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "news['text_clean'] = news['text'].map(lambda x: re.sub('[^a-zA-Z0-9 @ . , : - _]', '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crawled</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-02T04:24:51.072+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>QR Code Link to This Post All maintenance rece...</td>\n",
       "      <td>Dependable truck 03 Toyota Tacoma Double Cab $...</td>\n",
       "      <td>QR Code Link to This Post All maintenance rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-02T04:27:15.000+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>0 \\nNEW YORK: Automakers reported mixed US car...</td>\n",
       "      <td>US car sales mixed in January; trucks stay strong</td>\n",
       "      <td>0 NEW YORK: Automakers reported mixed US car s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-02T04:34:00.008+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>transmission: automatic   2005 Toyota Camry LE...</td>\n",
       "      <td>2005 TOYOTA CAMRY LE 167300 MILEAGE $2450 (TAL...</td>\n",
       "      <td>transmission: automatic   2005 Toyota Camry LE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-02T04:36:42.006+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>favorite this post Brand New Toyota Avalon Flo...</td>\n",
       "      <td>Brand New Toyota Avalon Floor Mats (New Britai...</td>\n",
       "      <td>favorite this post Brand New Toyota Avalon Flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-02T04:38:24.018+02:00</td>\n",
       "      <td>english</td>\n",
       "      <td>more ads by this user QR Code Link to This Pos...</td>\n",
       "      <td>2016 Lexus ES 350 (Coliseum Lexus of Oakland) ...</td>\n",
       "      <td>more ads by this user QR Code Link to This Pos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         crawled language  \\\n",
       "0  2018-02-02T04:24:51.072+02:00  english   \n",
       "1  2018-02-02T04:27:15.000+02:00  english   \n",
       "2  2018-02-02T04:34:00.008+02:00  english   \n",
       "3  2018-02-02T04:36:42.006+02:00  english   \n",
       "4  2018-02-02T04:38:24.018+02:00  english   \n",
       "\n",
       "                                                text  \\\n",
       "0  QR Code Link to This Post All maintenance rece...   \n",
       "1  0 \\nNEW YORK: Automakers reported mixed US car...   \n",
       "2  transmission: automatic   2005 Toyota Camry LE...   \n",
       "3  favorite this post Brand New Toyota Avalon Flo...   \n",
       "4  more ads by this user QR Code Link to This Pos...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Dependable truck 03 Toyota Tacoma Double Cab $...   \n",
       "1  US car sales mixed in January; trucks stay strong   \n",
       "2  2005 TOYOTA CAMRY LE 167300 MILEAGE $2450 (TAL...   \n",
       "3  Brand New Toyota Avalon Floor Mats (New Britai...   \n",
       "4  2016 Lexus ES 350 (Coliseum Lexus of Oakland) ...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  QR Code Link to This Post All maintenance rece...  \n",
       "1  0 NEW YORK: Automakers reported mixed US car s...  \n",
       "2  transmission: automatic   2005 Toyota Camry LE...  \n",
       "3  favorite this post Brand New Toyota Avalon Flo...  \n",
       "4  more ads by this user QR Code Link to This Pos...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# http://stevenloria.com/finding-important-words-in-a-document-using-tf-idf/\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "# tf(word, blob) computes \"term frequency\" which is the number of times a word appears in a document blob, \n",
    "# normalized by dividing by the total number of words in blob. We use TextBlob for breaking up the text into words \n",
    "# and getting the word counts.\n",
    "\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "# n_containing(word, bloblist) returns the number of documents containing word. \n",
    "# A generator expression is passed to the sum() function.\n",
    "\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "# idf(word, bloblist) computes \"inverse document frequency\" which measures how common a word is \n",
    "# among all documents in bloblist. The more common a word is, the lower its idf. \n",
    "# We take the ratio of the total number of documents to the number of documents containing word, \n",
    "# then take the log of that. Add 1 to the divisor to prevent division by zero\n",
    "\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "# tfidf(word, blob, bloblist) computes the TF-IDF score. It is simply the product of tf and idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloblist = []\n",
    "del bloblist[:]\n",
    "\n",
    "for i  in range(0,len(news)):\n",
    "    bloblist.append(TextBlob(news['text_clean'].iloc[i]))\n",
    "    \n",
    "len(bloblist)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in news 1\n",
      "\tWord: receipts, TF-IDF: 0.21733\n",
      "\tWord: Cash, TF-IDF: 0.21733\n",
      "\tWord: 6477478013, TF-IDF: 0.21733\n",
      "\tWord: sale, TF-IDF: 0.19481\n",
      "\tWord: maintenance, TF-IDF: 0.17883\n",
      "Top words in news 2\n",
      "\tWord: And, TF-IDF: 0.06643\n",
      "\tWord: In, TF-IDF: 0.05853\n",
      "\tWord: sales, TF-IDF: 0.04664\n",
      "\tWord: US, TF-IDF: 0.02365\n",
      "\tWord: The, TF-IDF: 0.02218\n",
      "Top words in news 3\n",
      "\tWord: AUTOMATIC, TF-IDF: 0.18935\n",
      "\tWord: automatic, TF-IDF: 0.15643\n",
      "\tWord: LE, TF-IDF: 0.11506\n",
      "\tWord: cyl, TF-IDF: 0.11506\n",
      "\tWord: VERY, TF-IDF: 0.11506\n",
      "Top words in news 4\n",
      "\tWord: Mats, TF-IDF: 0.13336\n",
      "\tWord: mats, TF-IDF: 0.13336\n",
      "\tWord: Floor, TF-IDF: 0.08891\n",
      "\tWord: floor, TF-IDF: 0.07969\n",
      "\tWord: Avalon, TF-IDF: 0.06394\n",
      "Top words in news 5\n",
      "\tWord: included, TF-IDF: 0.0788\n",
      "\tWord: Black, TF-IDF: 0.06732\n",
      "\tWord: Lexus, TF-IDF: 0.06177\n",
      "\tWord: below, TF-IDF: 0.05174\n",
      "\tWord: user, TF-IDF: 0.04396\n"
     ]
    }
   ],
   "source": [
    "for i, blob in enumerate(bloblist):\n",
    "# Print top 5 values\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(\"Top words in news {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:5]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = news['text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_clean = [clean(doc).split() for doc in news_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(news_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "%time oc_term_matrix = [dictionary.doc2bow(doc) for doc in news_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "numtopics = 3\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "%time ldamodel = Lda(doc_term_matrix, num_topics=numtopics, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.024*\"percent\" + 0.021*\"u\" + 0.013*\"cent\"')\n",
      "\n",
      "(1, '0.019*\"toyota\" + 0.007*\"japan\" + 0.006*\"vehicle\"')\n",
      "\n",
      "(2, '0.011*\"toyota\" + 0.009*\"sale\" + 0.008*\"vehicle\"')\n"
     ]
    }
   ],
   "source": [
    "print(*ldamodel.print_topics(num_topics=numtopics, num_words=3), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.024*\"percent\" + 0.021*\"u\" + 0.013*\"cent\" + 0.012*\"earnings\" + 0.012*\"per\"')\n",
      "\n",
      "(1, '0.019*\"toyota\" + 0.007*\"japan\" + 0.006*\"vehicle\" + 0.005*\"car\" + 0.005*\"also\"')\n",
      "\n",
      "(2, '0.011*\"toyota\" + 0.009*\"sale\" + 0.008*\"vehicle\" + 0.007*\"ford\" + 0.007*\"car\"')\n"
     ]
    }
   ],
   "source": [
    "print(*ldamodel.print_topics(num_topics=numtopics, num_words=5), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.024*\"percent\" + 0.021*\"u\" + 0.013*\"cent\" + 0.012*\"earnings\" + 0.012*\"per\" + 0.012*\"yield\" + 0.011*\"index\" + 0.010*\"share\" + 0.009*\"lower\" + 0.009*\"investor\"')\n",
      "\n",
      "(1, '0.019*\"toyota\" + 0.007*\"japan\" + 0.006*\"vehicle\" + 0.005*\"car\" + 0.005*\"also\" + 0.005*\"1\" + 0.005*\"2018\" + 0.004*\"one\" + 0.004*\"lexus\" + 0.003*\"canada\"')\n",
      "\n",
      "(2, '0.011*\"toyota\" + 0.009*\"sale\" + 0.008*\"vehicle\" + 0.007*\"ford\" + 0.007*\"car\" + 0.007*\"year\" + 0.006*\"new\" + 0.005*\"percent\" + 0.005*\"unit\" + 0.005*\"market\"')\n"
     ]
    }
   ],
   "source": [
    "print(*ldamodel.print_topics(num_topics=numtopics, num_words=10), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "numtopics = 5\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "%time ldamodel = Lda(doc_term_matrix, num_topics=numtopics, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.021*\"unit\" + 0.015*\"toyota\" + 0.014*\"vehicle\"')\n",
      "\n",
      "(1, '0.016*\"toyota\" + 0.008*\"vehicle\" + 0.007*\"post\"')\n",
      "\n",
      "(2, '0.012*\"toyota\" + 0.011*\"car\" + 0.005*\"job\"')\n",
      "\n",
      "(3, '0.024*\"percent\" + 0.021*\"u\" + 0.013*\"earnings\"')\n",
      "\n",
      "(4, '0.017*\"sale\" + 0.015*\"percent\" + 0.015*\"ford\"')\n"
     ]
    }
   ],
   "source": [
    "print(*ldamodel.print_topics(num_topics=numtopics, num_words=3), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.021*\"unit\" + 0.015*\"toyota\" + 0.014*\"vehicle\" + 0.012*\"market\" + 0.010*\"january\"')\n",
      "\n",
      "(1, '0.016*\"toyota\" + 0.008*\"vehicle\" + 0.007*\"post\" + 0.006*\"japan\" + 0.006*\"car\"')\n",
      "\n",
      "(2, '0.012*\"toyota\" + 0.011*\"car\" + 0.005*\"job\" + 0.005*\"company\" + 0.005*\"state\"')\n",
      "\n",
      "(3, '0.024*\"percent\" + 0.021*\"u\" + 0.013*\"earnings\" + 0.013*\"yield\" + 0.011*\"index\"')\n",
      "\n",
      "(4, '0.017*\"sale\" + 0.015*\"percent\" + 0.015*\"ford\" + 0.010*\"year\" + 0.010*\"toyota\"')\n"
     ]
    }
   ],
   "source": [
    "print(*ldamodel.print_topics(num_topics=numtopics, num_words=5), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.021*\"unit\" + 0.015*\"toyota\" + 0.014*\"vehicle\" + 0.012*\"market\" + 0.010*\"january\" + 0.009*\"new\" + 0.009*\"sale\" + 0.007*\"month\" + 0.006*\"2018\" + 0.006*\"share\"')\n",
      "\n",
      "(1, '0.016*\"toyota\" + 0.008*\"vehicle\" + 0.007*\"post\" + 0.006*\"japan\" + 0.006*\"car\" + 0.005*\"hydrogen\" + 0.005*\"year\" + 0.005*\"australia\" + 0.005*\"new\" + 0.004*\"contact\"')\n",
      "\n",
      "(2, '0.012*\"toyota\" + 0.011*\"car\" + 0.005*\"job\" + 0.005*\"company\" + 0.005*\"state\" + 0.005*\"d\" + 0.004*\"workforce\" + 0.004*\"said\" + 0.004*\"alabama\" + 0.004*\"model\"')\n",
      "\n",
      "(3, '0.024*\"percent\" + 0.021*\"u\" + 0.013*\"earnings\" + 0.013*\"yield\" + 0.011*\"index\" + 0.010*\"share\" + 0.010*\"lower\" + 0.010*\"cent\" + 0.009*\"per\" + 0.009*\"investor\"')\n",
      "\n",
      "(4, '0.017*\"sale\" + 0.015*\"percent\" + 0.015*\"ford\" + 0.010*\"year\" + 0.010*\"toyota\" + 0.007*\"january\" + 0.006*\"said\" + 0.006*\"motor\" + 0.006*\"company\" + 0.005*\"vehicle\"')\n"
     ]
    }
   ],
   "source": [
    "print(*ldamodel.print_topics(num_topics=numtopics, num_words=10), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "I chose 5 topics with 10 words. We can see quite well what the topic of the news around Toyota represented. Typically sales data that is lost within the smaller model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
